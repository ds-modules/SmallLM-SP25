{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4f8bed4",
   "metadata": {},
   "source": [
    "# GPT4All - setup by downloading models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d53b1d",
   "metadata": {},
   "source": [
    "##  Teaching LLM workflow by using open source models and GPT4All\n",
    "GPT4all is a framework to source models and handle Jupyter workflow and have them  work within the confines of limited compute eg like a personal computer or a cloud based server like we use for instruction.  \n",
    "\n",
    "### Shared Filesystem \n",
    "\n",
    "In the setup where I was teaching I used this notebook to download models from Huggingface and I put them in a shared-readwrite folder, where the students could access them on Jupyterhub.  I was using a Jupyterhub for teaching that had a shared folder system.  \n",
    "\n",
    "Your use case may vary \n",
    "- shared read write\n",
    "- each student downloads own models\n",
    "- download models to local "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4031df85-3599-4e95-a59d-f476cb36decd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that your python environment has gpt4all package installed.\n",
    "try:\n",
    "    from gpt4all import GPT4All\n",
    "except ImportError:\n",
    "    %pip install gpt4all\n",
    "    from gpt4all import GPT4All\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93d330d",
   "metadata": {},
   "source": [
    "## Which model to download\n",
    "In the use case for teaching on a Juptyerhub with a CPU, I was looking for **small models**, \n",
    " - ~1bn parameters\n",
    " - quantized (Weights have 4 decimal places instead of 10 )\n",
    "\n",
    "\n",
    "(This info is as of the writing of this notebook in May/June 2025 and this info is changing rapidly) \n",
    "\n",
    "\n",
    "You can explore the world of models at :\n",
    "[Hugging Face Model List](https://huggingface.co/models)\n",
    "\n",
    "GPT4All is using a subset of these models - Here is the description from their [documentation](https://docs.gpt4all.io/gpt4all_desktop/models.html#explore-models):\n",
    "\n",
    "- Many LLMs are available at various sizes, quantizations, and licenses.\n",
    "\n",
    "- LLMs with more parameters tend to be better at coherently responding to instructions\n",
    "\n",
    "- LLMs with a smaller quantization (e.g. 4bit instead of 16bit) are much faster and less memory intensive, and tend to have slightly worse performance\n",
    "\n",
    "- Licenses vary in their terms for personal and commercial use\n",
    "\n",
    "\n",
    "\n",
    "Five that I picked to download are:\n",
    "- `DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf`\n",
    "- `Phi-3-mini-4k-instruct.Q4_0.gguf`\n",
    "- `Llama-3.2-1B-Instruct-Q4_0.gguf`\t\t \n",
    "- `qwen2-1_5b-instruct-q4_0.gguf`\n",
    "- `mistral-7b-instruct-v0.1.Q4_0.gguf`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273601bd",
   "metadata": {},
   "source": [
    "The simplest way to download a model is just to call for it in GPT4All and then it downloads it if you dont have it\n",
    "\n",
    "Don't worry if you get `llama_model_load: error loading model: error loading model vocabulary: unknown pre-tokenizer type: 'deepseek-r1-qwen'`  thats about access to GPUs which we don't have in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f784d27-ea55-4a2f-86e7-d08ff0e5f2f1",
   "metadata": {},
   "source": [
    "## Let's check out our local filesystem path and where we will download the files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0005c223-3f01-4478-92d6-3c2de6bb1a9d",
   "metadata": {},
   "source": [
    "### Approach 1 -  if a Shared Hub is being used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1475f454-9d2a-45c2-9b98-11e3ee82304b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This only worked for SP 25 instuction on Berkeley Datahub\n",
    "#!ls /home/jovyan/_shared/econ148-readwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e054e688-fa14-4990-aa56-be9aea92614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cal-ICOR workhop Hub?\n",
    "#!ls /home/jovyan/shared-rw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595269ab-c7c0-4da5-8110-2907dfc36d86",
   "metadata": {},
   "source": [
    "### Approach 2 -  if a local machine is being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "600fff6e-0a22-4d6f-ae3e-8c4c28d6d3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mcourse\u001b[m\u001b[m                        qwen2-1_5b-instruct-q4_0.gguf\n",
      "gemma-2b-it.Q4_0.gguf\n"
     ]
    }
   ],
   "source": [
    "#This is my local path to a directory called shared-rw\n",
    "!ls shared-rw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "640f0889-5886-40ad-bd60-df4745851d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf orca-mini-3b-gguf2-q4_0.gguf\n",
      "\u001b[34mcourse\u001b[m\u001b[m                                  qwen2-1_5b-instruct-q4_0.gguf\n",
      "gemma-2b-it.Q4_0.gguf\n"
     ]
    }
   ],
   "source": [
    "# or the full path ( this is on my laptop) \n",
    "!ls /Users/ericvandusen/Documents/GitHub/SmallLM-SP25/shared-rw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c238eb73-62e8-4cee-aeb3-34a81be11218",
   "metadata": {},
   "source": [
    "### Set the path where the models will download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be5d6020-693b-4bef-8810-de0d21675bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/Users/ericvandusen/Documents/GitHub/SmallLM-SP25/shared-rw\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87d72df-194d-4209-b728-2ae92f5273f6",
   "metadata": {},
   "source": [
    "## Downloading the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5f8a73d-e34f-4c52-bd7f-5e70f0b869d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|████████████████████████| 1.07G/1.07G [00:14<00:00, 73.3MiB/s]\n",
      "Model downloaded to '/Users/ericvandusen/Documents/GitHub/SmallLM-SP25/shared-rw/DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf'\n",
      "llama_model_load: error loading model: error loading model vocabulary: unknown pre-tokenizer type: 'deepseek-r1-qwen'\n",
      "llama_load_model_from_file: failed to load model\n",
      "LLAMA ERROR: failed to load model from /Users/ericvandusen/Documents/GitHub/SmallLM-SP25/shared-rw/DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf\n"
     ]
    }
   ],
   "source": [
    "# Define the \"model\" object to which this notebook's code will send conversations & prompts\n",
    "model = GPT4All(\n",
    "    model_name=\"DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf\",\n",
    "    allow_download=True,\n",
    "    model_path=path,\n",
    "    verbose=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e739378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef346506-e0ee-4f13-be1f-a2090850f57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|████████████████████████| 1.98G/1.98G [00:38<00:00, 50.9MiB/s]\n",
      "Verifying: 100%|███████████████████████████| 1.98G/1.98G [00:02<00:00, 818MiB/s]\n",
      "Model downloaded to '/Users/ericvandusen/Documents/GitHub/SmallLM-SP25/shared-rw/orca-mini-3b-gguf2-q4_0.gguf'\n"
     ]
    }
   ],
   "source": [
    "# Define the \"model\" object to which this notebook's code will send conversations & prompts\n",
    "model = GPT4All(\n",
    "    model_name=\"orca-mini-3b-gguf2-q4_0.gguf\",\n",
    "    allow_download=True,\n",
    "    model_path=path,\n",
    "    verbose=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c741305-e39c-4251-9347-c30f18e80f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "course\n",
      "qwen2-1_5b-instruct-q4_0.gguf\n",
      "gemma-2b-it.Q4_0.gguf\n",
      "DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf\n",
      "orca-mini-3b-gguf2-q4_0.gguf\n"
     ]
    }
   ],
   "source": [
    "# Show models available in the Hub shared directory. Larger models may run slowly, or not at all.\n",
    "import os\n",
    "print(\"\\n\".join(os.listdir(path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d37c9e6-936a-40be-b9a5-a213552b1b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf orca-mini-3b-gguf2-q4_0.gguf\n",
      "\u001b[34mcourse\u001b[m\u001b[m                                  qwen2-1_5b-instruct-q4_0.gguf\n",
      "gemma-2b-it.Q4_0.gguf\n"
     ]
    }
   ],
   "source": [
    "!ls \"{path}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f856c4d5",
   "metadata": {},
   "source": [
    "## Bonus Searching for models from a database \n",
    "\n",
    " - We can go to GPT4All database\n",
    " - Make that database into a pandas dataframe\n",
    " - Filter to pick nodels we want\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06680376-15c4-4cf3-aecb-c802e474352a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9c9a86bc-c0a5-461b-bb3a-5f1a33d89398",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load JSON from the GPT4All models repository\n",
    "#Small curated list\n",
    "url = \"https://gpt4all.io/models/models3.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "45ce2234",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = requests.get(url).json()\n",
    "# Convert to DataFrame\n",
    "Models_df = pd.DataFrame(models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614239e5-eb14-4594-9d24-c7c3c0fda472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bceec60b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order</th>\n",
       "      <th>md5sum</th>\n",
       "      <th>name</th>\n",
       "      <th>filename</th>\n",
       "      <th>filesize</th>\n",
       "      <th>requires</th>\n",
       "      <th>ramrequired</th>\n",
       "      <th>parameters</th>\n",
       "      <th>quant</th>\n",
       "      <th>type</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>chatTemplate</th>\n",
       "      <th>systemPrompt</th>\n",
       "      <th>promptTemplate</th>\n",
       "      <th>sha256sum</th>\n",
       "      <th>systemMessage</th>\n",
       "      <th>removedIn</th>\n",
       "      <th>disableGUI</th>\n",
       "      <th>embeddingModel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>a54c08a7b90e4029a8c2ab5b5dc936aa</td>\n",
       "      <td>Reasoner v1</td>\n",
       "      <td>qwen2.5-coder-7b-instruct-q4_0.gguf</td>\n",
       "      <td>4431390720</td>\n",
       "      <td>3.6.0</td>\n",
       "      <td>8</td>\n",
       "      <td>8 billion</td>\n",
       "      <td>q4_0</td>\n",
       "      <td>qwen2</td>\n",
       "      <td>&lt;ul&gt;&lt;li&gt;Based on &lt;a href=\"https://huggingface....</td>\n",
       "      <td>https://huggingface.co/Qwen/Qwen2.5-Coder-7B-I...</td>\n",
       "      <td>{{- '&lt;|im_start|&gt;system\\n' }}\\n{% if toolList|...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aa</td>\n",
       "      <td>c87ad09e1e4c8f9c35a5fcef52b6f1c9</td>\n",
       "      <td>Llama 3 8B Instruct</td>\n",
       "      <td>Meta-Llama-3-8B-Instruct.Q4_0.gguf</td>\n",
       "      <td>4661724384</td>\n",
       "      <td>2.7.1</td>\n",
       "      <td>8</td>\n",
       "      <td>8 billion</td>\n",
       "      <td>q4_0</td>\n",
       "      <td>LLaMA3</td>\n",
       "      <td>&lt;ul&gt;&lt;li&gt;Fast responses&lt;/li&gt;&lt;li&gt;Chat based mode...</td>\n",
       "      <td>https://gpt4all.io/models/gguf/Meta-Llama-3-8B...</td>\n",
       "      <td>{%- set loop_messages = messages %}\\n{%- for m...</td>\n",
       "      <td></td>\n",
       "      <td>&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\n%1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aa1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DeepSeek-R1-Distill-Qwen-7B</td>\n",
       "      <td>DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf</td>\n",
       "      <td>4444121056</td>\n",
       "      <td>3.8.0</td>\n",
       "      <td>8</td>\n",
       "      <td>7 billion</td>\n",
       "      <td>q4_0</td>\n",
       "      <td>deepseek</td>\n",
       "      <td>&lt;p&gt;The official Qwen2.5-Math-7B distillation o...</td>\n",
       "      <td>https://huggingface.co/bartowski/DeepSeek-R1-D...</td>\n",
       "      <td>{%- if not add_generation_prompt is defined %}...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5cd4ee65211770f1d99b4f6f4951780b9ef40e29314bd6...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aa2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DeepSeek-R1-Distill-Qwen-14B</td>\n",
       "      <td>DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf</td>\n",
       "      <td>8544267680</td>\n",
       "      <td>3.8.0</td>\n",
       "      <td>16</td>\n",
       "      <td>14 billion</td>\n",
       "      <td>q4_0</td>\n",
       "      <td>deepseek</td>\n",
       "      <td>&lt;p&gt;The official Qwen2.5-14B distillation of De...</td>\n",
       "      <td>https://huggingface.co/bartowski/DeepSeek-R1-D...</td>\n",
       "      <td>{%- if not add_generation_prompt is defined %}...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>906b3382f2680f4ce845459b4a122e904002b075238080...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aa3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DeepSeek-R1-Distill-Llama-8B</td>\n",
       "      <td>DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf</td>\n",
       "      <td>4675894112</td>\n",
       "      <td>3.8.0</td>\n",
       "      <td>8</td>\n",
       "      <td>8 billion</td>\n",
       "      <td>q4_0</td>\n",
       "      <td>deepseek</td>\n",
       "      <td>&lt;p&gt;The official Llama-3.1-8B distillation of D...</td>\n",
       "      <td>https://huggingface.co/bartowski/DeepSeek-R1-D...</td>\n",
       "      <td>{%- if not add_generation_prompt is defined %}...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0eb93e436ac8beec18aceb958c120d282cb2cf5451b231...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aa4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DeepSeek-R1-Distill-Qwen-1.5B</td>\n",
       "      <td>DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf</td>\n",
       "      <td>1068807776</td>\n",
       "      <td>3.8.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.5 billion</td>\n",
       "      <td>q4_0</td>\n",
       "      <td>deepseek</td>\n",
       "      <td>&lt;p&gt;The official Qwen2.5-Math-1.5B distillation...</td>\n",
       "      <td>https://huggingface.co/bartowski/DeepSeek-R1-D...</td>\n",
       "      <td>{%- if not add_generation_prompt is defined %}...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b3af887d0a015b39fab2395e4faf682c1a81a6a3fd09a4...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>b</td>\n",
       "      <td>27b44e8ae1817525164ddf4f8dae8af4</td>\n",
       "      <td>Llama 3.2 3B Instruct</td>\n",
       "      <td>Llama-3.2-3B-Instruct-Q4_0.gguf</td>\n",
       "      <td>1921909280</td>\n",
       "      <td>3.4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3 billion</td>\n",
       "      <td>q4_0</td>\n",
       "      <td>LLaMA3</td>\n",
       "      <td>&lt;ul&gt;&lt;li&gt;Fast responses&lt;/li&gt;&lt;li&gt;Instruct model&lt;...</td>\n",
       "      <td>https://huggingface.co/bartowski/Llama-3.2-3B-...</td>\n",
       "      <td>{{- bos_token }}\\n{%- set date_string = strfti...</td>\n",
       "      <td>&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\nCu...</td>\n",
       "      <td>&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\n%1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>c</td>\n",
       "      <td>48ff0243978606fdba19d899b77802fc</td>\n",
       "      <td>Llama 3.2 1B Instruct</td>\n",
       "      <td>Llama-3.2-1B-Instruct-Q4_0.gguf</td>\n",
       "      <td>773025920</td>\n",
       "      <td>3.4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1 billion</td>\n",
       "      <td>q4_0</td>\n",
       "      <td>LLaMA3</td>\n",
       "      <td>&lt;ul&gt;&lt;li&gt;Fast responses&lt;/li&gt;&lt;li&gt;Instruct model&lt;...</td>\n",
       "      <td>https://huggingface.co/bartowski/Llama-3.2-1B-...</td>\n",
       "      <td>{{- bos_token }}\\n{%- set date_string = strfti...</td>\n",
       "      <td>&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\nCu...</td>\n",
       "      <td>&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\n%1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>d</td>\n",
       "      <td>a5f6b4eabd3992da4d7fb7f020f921eb</td>\n",
       "      <td>Nous Hermes 2 Mistral DPO</td>\n",
       "      <td>Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf</td>\n",
       "      <td>4108928000</td>\n",
       "      <td>2.7.1</td>\n",
       "      <td>8</td>\n",
       "      <td>7 billion</td>\n",
       "      <td>q4_0</td>\n",
       "      <td>Mistral</td>\n",
       "      <td>&lt;strong&gt;Good overall fast chat model&lt;/strong&gt;&lt;...</td>\n",
       "      <td>https://huggingface.co/NousResearch/Nous-Herme...</td>\n",
       "      <td>{%- for message in messages %}\\n    {{- '&lt;|im_...</td>\n",
       "      <td></td>\n",
       "      <td>&lt;|im_start|&gt;user\\n%1&lt;|im_end|&gt;\\n&lt;|im_start|&gt;as...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>e</td>\n",
       "      <td>97463be739b50525df56d33b26b00852</td>\n",
       "      <td>Mistral Instruct</td>\n",
       "      <td>mistral-7b-instruct-v0.1.Q4_0.gguf</td>\n",
       "      <td>4108916384</td>\n",
       "      <td>2.5.0</td>\n",
       "      <td>8</td>\n",
       "      <td>7 billion</td>\n",
       "      <td>q4_0</td>\n",
       "      <td>Mistral</td>\n",
       "      <td>&lt;strong&gt;Strong overall fast instruction follow...</td>\n",
       "      <td>https://gpt4all.io/models/gguf/mistral-7b-inst...</td>\n",
       "      <td>{%- if messages[0]['role'] == 'system' %}\\n   ...</td>\n",
       "      <td></td>\n",
       "      <td>[INST] %1 [/INST]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>f</td>\n",
       "      <td>8a9c75bcd8a66b7693f158ec96924eeb</td>\n",
       "      <td>Llama 3.1 8B Instruct 128k</td>\n",
       "      <td>Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf</td>\n",
       "      <td>4661212096</td>\n",
       "      <td>3.1.1</td>\n",
       "      <td>8</td>\n",
       "      <td>8 billion</td>\n",
       "      <td>q4_0</td>\n",
       "      <td>LLaMA3</td>\n",
       "      <td>&lt;ul&gt;&lt;li&gt;&lt;strong&gt;For advanced users only. Not r...</td>\n",
       "      <td>https://huggingface.co/GPT4All-Community/Meta-...</td>\n",
       "      <td>{%- set loop_messages = messages %}\\n{%- for m...</td>\n",
       "      <td>&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\nCu...</td>\n",
       "      <td>&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\n%1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>g</td>\n",
       "      <td>f692417a22405d80573ac10cb0cd6c6a</td>\n",
       "      <td>Mistral OpenOrca</td>\n",
       "      <td>mistral-7b-openorca.gguf2.Q4_0.gguf</td>\n",
       "      <td>4108928128</td>\n",
       "      <td>2.7.1</td>\n",
       "      <td>8</td>\n",
       "      <td>7 billion</td>\n",
       "      <td>q4_0</td>\n",
       "      <td>Mistral</td>\n",
       "      <td>&lt;strong&gt;Strong overall fast chat model&lt;/strong...</td>\n",
       "      <td>https://gpt4all.io/models/gguf/mistral-7b-open...</td>\n",
       "      <td>{%- for message in messages %}\\n    {{- '&lt;|im_...</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are MistralOrca, a lar...</td>\n",
       "      <td>&lt;|im_start|&gt;user\\n%1&lt;|im_end|&gt;\\n&lt;|im_start|&gt;as...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>h</td>\n",
       "      <td>c4c78adf744d6a20f05c8751e3961b84</td>\n",
       "      <td>GPT4All Falcon</td>\n",
       "      <td>gpt4all-falcon-newbpe-q4_0.gguf</td>\n",
       "      <td>4210994112</td>\n",
       "      <td>2.6.0</td>\n",
       "      <td>8</td>\n",
       "      <td>7 billion</td>\n",
       "      <td>q4_0</td>\n",
       "      <td>Falcon</td>\n",
       "      <td>&lt;strong&gt;Very fast model with good quality&lt;/str...</td>\n",
       "      <td>https://gpt4all.io/models/gguf/gpt4all-falcon-...</td>\n",
       "      <td>{%- if messages[0]['role'] == 'system' %}\\n   ...</td>\n",
       "      <td></td>\n",
       "      <td>### Instruction:\\n%1\\n\\n### Response:\\n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>i</td>\n",
       "      <td>00c8593ba57f5240f59662367b3ed4a5</td>\n",
       "      <td>Orca 2 (Medium)</td>\n",
       "      <td>orca-2-7b.Q4_0.gguf</td>\n",
       "      <td>3825824192</td>\n",
       "      <td>2.5.2</td>\n",
       "      <td>8</td>\n",
       "      <td>7 billion</td>\n",
       "      <td>q4_0</td>\n",
       "      <td>LLaMA2</td>\n",
       "      <td>&lt;ul&gt;&lt;li&gt;Instruction based&lt;li&gt;Trained by Micros...</td>\n",
       "      <td>https://gpt4all.io/models/gguf/orca-2-7b.Q4_0....</td>\n",
       "      <td>{%- for message in messages %}\\n    {{- '&lt;|im_...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>j</td>\n",
       "      <td>3c0d63c4689b9af7baa82469a6f51a19</td>\n",
       "      <td>Orca 2 (Full)</td>\n",
       "      <td>orca-2-13b.Q4_0.gguf</td>\n",
       "      <td>7365856064</td>\n",
       "      <td>2.5.2</td>\n",
       "      <td>16</td>\n",
       "      <td>13 billion</td>\n",
       "      <td>q4_0</td>\n",
       "      <td>LLaMA2</td>\n",
       "      <td>&lt;ul&gt;&lt;li&gt;Instruction based&lt;li&gt;Trained by Micros...</td>\n",
       "      <td>https://gpt4all.io/models/gguf/orca-2-13b.Q4_0...</td>\n",
       "      <td>{%- for message in messages %}\\n    {{- '&lt;|im_...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>k</td>\n",
       "      <td>5aff90007499bce5c64b1c0760c0b186</td>\n",
       "      <td>Wizard v1.2</td>\n",
       "      <td>wizardlm-13b-v1.2.Q4_0.gguf</td>\n",
       "      <td>7365834624</td>\n",
       "      <td>2.5.0</td>\n",
       "      <td>16</td>\n",
       "      <td>13 billion</td>\n",
       "      <td>q4_0</td>\n",
       "      <td>LLaMA2</td>\n",
       "      <td>&lt;strong&gt;Strong overall larger model&lt;/strong&gt;&lt;b...</td>\n",
       "      <td>https://gpt4all.io/models/gguf/wizardlm-13b-v1...</td>\n",
       "      <td>{%- if messages[0]['role'] == 'system' %}\\n   ...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A chat between a curious user and an artificia...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>l</td>\n",
       "      <td>31b47b4e8c1816b62684ac3ca373f9e1</td>\n",
       "      <td>Ghost 7B v0.9.1</td>\n",
       "      <td>ghost-7b-v0.9.1-Q4_0.gguf</td>\n",
       "      <td>4108916960</td>\n",
       "      <td>2.7.1</td>\n",
       "      <td>8</td>\n",
       "      <td>7 billion</td>\n",
       "      <td>q4_0</td>\n",
       "      <td>Mistral</td>\n",
       "      <td>&lt;strong&gt;Ghost 7B v0.9.1&lt;/strong&gt; fast, powerfu...</td>\n",
       "      <td>https://huggingface.co/lamhieu/ghost-7b-v0.9.1...</td>\n",
       "      <td>{%- for message in messages %}\\n    {%- if mes...</td>\n",
       "      <td>&lt;|system|&gt;\\nYou are Ghost created by Lam Hieu....</td>\n",
       "      <td>&lt;|user|&gt;\\n%1&lt;/s&gt;\\n&lt;|assistant|&gt;\\n%2&lt;/s&gt;\\n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>You are Ghost created by Lam Hieu. You are a h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>m</td>\n",
       "      <td>3d12810391d04d1153b692626c0c6e16</td>\n",
       "      <td>Hermes</td>\n",
       "      <td>nous-hermes-llama2-13b.Q4_0.gguf</td>\n",
       "      <td>7366062080</td>\n",
       "      <td>2.5.0</td>\n",
       "      <td>16</td>\n",
       "      <td>13 billion</td>\n",
       "      <td>q4_0</td>\n",
       "      <td>LLaMA2</td>\n",
       "      <td>&lt;strong&gt;Extremely good model&lt;/strong&gt;&lt;br&gt;&lt;ul&gt;&lt;...</td>\n",
       "      <td>https://gpt4all.io/models/gguf/nous-hermes-lla...</td>\n",
       "      <td>{%- if messages[0]['role'] == 'system' %}\\n   ...</td>\n",
       "      <td></td>\n",
       "      <td>### Instruction:\\n%1\\n\\n### Response:\\n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>n</td>\n",
       "      <td>40388eb2f8d16bb5d08c96fdfaac6b2c</td>\n",
       "      <td>Snoozy</td>\n",
       "      <td>gpt4all-13b-snoozy-q4_0.gguf</td>\n",
       "      <td>7365834624</td>\n",
       "      <td>2.5.0</td>\n",
       "      <td>16</td>\n",
       "      <td>13 billion</td>\n",
       "      <td>q4_0</td>\n",
       "      <td>LLaMA</td>\n",
       "      <td>&lt;strong&gt;Very good overall model&lt;/strong&gt;&lt;br&gt;&lt;u...</td>\n",
       "      <td>https://gpt4all.io/models/gguf/gpt4all-13b-sno...</td>\n",
       "      <td>{%- if messages[0]['role'] == 'system' %}\\n   ...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>o</td>\n",
       "      <td>15dcb4d7ea6de322756449c11a0b7545</td>\n",
       "      <td>MPT Chat</td>\n",
       "      <td>mpt-7b-chat-newbpe-q4_0.gguf</td>\n",
       "      <td>3912373472</td>\n",
       "      <td>2.7.1</td>\n",
       "      <td>8</td>\n",
       "      <td>7 billion</td>\n",
       "      <td>q4_0</td>\n",
       "      <td>MPT</td>\n",
       "      <td>&lt;strong&gt;Good model with novel architecture&lt;/st...</td>\n",
       "      <td>https://gpt4all.io/models/gguf/mpt-7b-chat-new...</td>\n",
       "      <td>{%- for message in messages %}\\n    {{- '&lt;|im_...</td>\n",
       "      <td>&lt;|im_start|&gt;system\\n- You are a helpful assist...</td>\n",
       "      <td>&lt;|im_start|&gt;user\\n%1&lt;|im_end|&gt;\\n&lt;|im_start|&gt;as...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.7.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>p</td>\n",
       "      <td>ab5d8e8a2f79365ea803c1f1d0aa749d</td>\n",
       "      <td>MPT Chat</td>\n",
       "      <td>mpt-7b-chat.gguf4.Q4_0.gguf</td>\n",
       "      <td>3796178112</td>\n",
       "      <td>2.7.3</td>\n",
       "      <td>8</td>\n",
       "      <td>7 billion</td>\n",
       "      <td>q4_0</td>\n",
       "      <td>MPT</td>\n",
       "      <td>&lt;strong&gt;Good model with novel architecture&lt;/st...</td>\n",
       "      <td>https://gpt4all.io/models/gguf/mpt-7b-chat.ggu...</td>\n",
       "      <td>{%- for message in messages %}\\n    {{- '&lt;|im_...</td>\n",
       "      <td>&lt;|im_start|&gt;system\\n- You are a helpful assist...</td>\n",
       "      <td>&lt;|im_start|&gt;user\\n%1&lt;|im_end|&gt;\\n&lt;|im_start|&gt;as...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>q</td>\n",
       "      <td>f8347badde9bfc2efbe89124d78ddaf5</td>\n",
       "      <td>Phi-3 Mini Instruct</td>\n",
       "      <td>Phi-3-mini-4k-instruct.Q4_0.gguf</td>\n",
       "      <td>2176181568</td>\n",
       "      <td>2.7.1</td>\n",
       "      <td>4</td>\n",
       "      <td>4 billion</td>\n",
       "      <td>q4_0</td>\n",
       "      <td>Phi-3</td>\n",
       "      <td>&lt;ul&gt;&lt;li&gt;Very fast responses&lt;/li&gt;&lt;li&gt;Chat based...</td>\n",
       "      <td>https://gpt4all.io/models/gguf/Phi-3-mini-4k-i...</td>\n",
       "      <td>{{- bos_token }}\\n{%- for message in messages ...</td>\n",
       "      <td></td>\n",
       "      <td>&lt;|user|&gt;\\n%1&lt;|end|&gt;\\n&lt;|assistant|&gt;\\n%2&lt;|end|&gt;\\n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>r</td>\n",
       "      <td>0e769317b90ac30d6e09486d61fefa26</td>\n",
       "      <td>Mini Orca (Small)</td>\n",
       "      <td>orca-mini-3b-gguf2-q4_0.gguf</td>\n",
       "      <td>1979946720</td>\n",
       "      <td>2.5.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3 billion</td>\n",
       "      <td>q4_0</td>\n",
       "      <td>OpenLLaMa</td>\n",
       "      <td>&lt;strong&gt;Small version of new model with novel ...</td>\n",
       "      <td>https://gpt4all.io/models/gguf/orca-mini-3b-gg...</td>\n",
       "      <td>{%- if messages[0]['role'] == 'system' %}\\n   ...</td>\n",
       "      <td>### System:\\nYou are an AI assistant that foll...</td>\n",
       "      <td>### User:\\n%1\\n\\n### Response:\\n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>s</td>\n",
       "      <td>c232f17e09bca4b7ee0b5b1f4107c01e</td>\n",
       "      <td>Replit</td>\n",
       "      <td>replit-code-v1_5-3b-newbpe-q4_0.gguf</td>\n",
       "      <td>1953055104</td>\n",
       "      <td>2.6.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3 billion</td>\n",
       "      <td>q4_0</td>\n",
       "      <td>Replit</td>\n",
       "      <td>&lt;strong&gt;Trained on subset of the Stack&lt;/strong...</td>\n",
       "      <td>https://gpt4all.io/models/gguf/replit-code-v1_...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>%1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>true</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>t</td>\n",
       "      <td>70841751ccd95526d3dcfa829e11cd4c</td>\n",
       "      <td>Starcoder</td>\n",
       "      <td>starcoder-newbpe-q4_0.gguf</td>\n",
       "      <td>8987411904</td>\n",
       "      <td>2.6.0</td>\n",
       "      <td>4</td>\n",
       "      <td>7 billion</td>\n",
       "      <td>q4_0</td>\n",
       "      <td>Starcoder</td>\n",
       "      <td>&lt;strong&gt;Trained on subset of the Stack&lt;/strong...</td>\n",
       "      <td>https://gpt4all.io/models/gguf/starcoder-newbp...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>%1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>true</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>u</td>\n",
       "      <td>e973dd26f0ffa6e46783feaea8f08c83</td>\n",
       "      <td>Rift coder</td>\n",
       "      <td>rift-coder-v0-7b-q4_0.gguf</td>\n",
       "      <td>3825903776</td>\n",
       "      <td>2.5.0</td>\n",
       "      <td>8</td>\n",
       "      <td>7 billion</td>\n",
       "      <td>q4_0</td>\n",
       "      <td>LLaMA</td>\n",
       "      <td>&lt;strong&gt;Trained on collection of Python and Ty...</td>\n",
       "      <td>https://gpt4all.io/models/gguf/rift-coder-v0-7...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>%1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>true</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>v</td>\n",
       "      <td>e479e6f38b59afc51a470d1953a6bfc7</td>\n",
       "      <td>SBert</td>\n",
       "      <td>all-MiniLM-L6-v2-f16.gguf</td>\n",
       "      <td>45887744</td>\n",
       "      <td>2.5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40 million</td>\n",
       "      <td>f16</td>\n",
       "      <td>Bert</td>\n",
       "      <td>&lt;strong&gt;LocalDocs text embeddings model&lt;/stron...</td>\n",
       "      <td>https://gpt4all.io/models/gguf/all-MiniLM-L6-v...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.7.4</td>\n",
       "      <td>true</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>w</td>\n",
       "      <td>dd90e2cb7f8e9316ac3796cece9883b5</td>\n",
       "      <td>SBert</td>\n",
       "      <td>all-MiniLM-L6-v2.gguf2.f16.gguf</td>\n",
       "      <td>45949216</td>\n",
       "      <td>2.7.4</td>\n",
       "      <td>1</td>\n",
       "      <td>40 million</td>\n",
       "      <td>f16</td>\n",
       "      <td>Bert</td>\n",
       "      <td>&lt;strong&gt;LocalDocs text embeddings model&lt;/stron...</td>\n",
       "      <td>https://gpt4all.io/models/gguf/all-MiniLM-L6-v...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>x</td>\n",
       "      <td>919de4dd6f25351bcb0223790db1932d</td>\n",
       "      <td>EM German Mistral</td>\n",
       "      <td>em_german_mistral_v01.Q4_0.gguf</td>\n",
       "      <td>4108916352</td>\n",
       "      <td>2.5.0</td>\n",
       "      <td>8</td>\n",
       "      <td>7 billion</td>\n",
       "      <td>q4_0</td>\n",
       "      <td>Mistral</td>\n",
       "      <td>&lt;strong&gt;Mistral-based model for German-languag...</td>\n",
       "      <td>https://huggingface.co/TheBloke/em_german_mist...</td>\n",
       "      <td>{%- if messages[0]['role'] == 'system' %}\\n   ...</td>\n",
       "      <td>Du bist ein hilfreicher Assistent.</td>\n",
       "      <td>USER: %1 ASSISTANT:</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Du bist ein hilfreicher Assistent.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>y</td>\n",
       "      <td>60ea031126f82db8ddbbfecc668315d2</td>\n",
       "      <td>Nomic Embed Text v1</td>\n",
       "      <td>nomic-embed-text-v1.f16.gguf</td>\n",
       "      <td>274290560</td>\n",
       "      <td>2.7.4</td>\n",
       "      <td>1</td>\n",
       "      <td>137 million</td>\n",
       "      <td>f16</td>\n",
       "      <td>Bert</td>\n",
       "      <td>nomic-embed-text-v1</td>\n",
       "      <td>https://gpt4all.io/models/gguf/nomic-embed-tex...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>true</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>z</td>\n",
       "      <td>a5401e7f7e46ed9fcaed5b60a281d547</td>\n",
       "      <td>Nomic Embed Text v1.5</td>\n",
       "      <td>nomic-embed-text-v1.5.f16.gguf</td>\n",
       "      <td>274290560</td>\n",
       "      <td>2.7.4</td>\n",
       "      <td>1</td>\n",
       "      <td>137 million</td>\n",
       "      <td>f16</td>\n",
       "      <td>Bert</td>\n",
       "      <td>nomic-embed-text-v1.5</td>\n",
       "      <td>https://gpt4all.io/models/gguf/nomic-embed-tex...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>true</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>zzz</td>\n",
       "      <td>a8c5a783105f87a481543d4ed7d7586d</td>\n",
       "      <td>Qwen2-1.5B-Instruct</td>\n",
       "      <td>qwen2-1_5b-instruct-q4_0.gguf</td>\n",
       "      <td>937532800</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.5 billion</td>\n",
       "      <td>q4_0</td>\n",
       "      <td>qwen2</td>\n",
       "      <td>&lt;ul&gt;&lt;li&gt;Very fast responses&lt;/li&gt;&lt;li&gt;Instructio...</td>\n",
       "      <td>https://huggingface.co/Qwen/Qwen2-1.5B-Instruc...</td>\n",
       "      <td>{%- for message in messages %}\\n    {%- if loo...</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nBelow is an instruction th...</td>\n",
       "      <td>&lt;|im_start|&gt;user\\n%1&lt;|im_end|&gt;\\n&lt;|im_start|&gt;as...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   order                            md5sum                           name  \\\n",
       "0      a  a54c08a7b90e4029a8c2ab5b5dc936aa                    Reasoner v1   \n",
       "1     aa  c87ad09e1e4c8f9c35a5fcef52b6f1c9            Llama 3 8B Instruct   \n",
       "2    aa1                               NaN    DeepSeek-R1-Distill-Qwen-7B   \n",
       "3    aa2                               NaN   DeepSeek-R1-Distill-Qwen-14B   \n",
       "4    aa3                               NaN   DeepSeek-R1-Distill-Llama-8B   \n",
       "5    aa4                               NaN  DeepSeek-R1-Distill-Qwen-1.5B   \n",
       "6      b  27b44e8ae1817525164ddf4f8dae8af4          Llama 3.2 3B Instruct   \n",
       "7      c  48ff0243978606fdba19d899b77802fc          Llama 3.2 1B Instruct   \n",
       "8      d  a5f6b4eabd3992da4d7fb7f020f921eb      Nous Hermes 2 Mistral DPO   \n",
       "9      e  97463be739b50525df56d33b26b00852               Mistral Instruct   \n",
       "10     f  8a9c75bcd8a66b7693f158ec96924eeb     Llama 3.1 8B Instruct 128k   \n",
       "11     g  f692417a22405d80573ac10cb0cd6c6a               Mistral OpenOrca   \n",
       "12     h  c4c78adf744d6a20f05c8751e3961b84                 GPT4All Falcon   \n",
       "13     i  00c8593ba57f5240f59662367b3ed4a5                Orca 2 (Medium)   \n",
       "14     j  3c0d63c4689b9af7baa82469a6f51a19                  Orca 2 (Full)   \n",
       "15     k  5aff90007499bce5c64b1c0760c0b186                    Wizard v1.2   \n",
       "16     l  31b47b4e8c1816b62684ac3ca373f9e1                Ghost 7B v0.9.1   \n",
       "17     m  3d12810391d04d1153b692626c0c6e16                         Hermes   \n",
       "18     n  40388eb2f8d16bb5d08c96fdfaac6b2c                         Snoozy   \n",
       "19     o  15dcb4d7ea6de322756449c11a0b7545                       MPT Chat   \n",
       "20     p  ab5d8e8a2f79365ea803c1f1d0aa749d                       MPT Chat   \n",
       "21     q  f8347badde9bfc2efbe89124d78ddaf5            Phi-3 Mini Instruct   \n",
       "22     r  0e769317b90ac30d6e09486d61fefa26              Mini Orca (Small)   \n",
       "23     s  c232f17e09bca4b7ee0b5b1f4107c01e                         Replit   \n",
       "24     t  70841751ccd95526d3dcfa829e11cd4c                      Starcoder   \n",
       "25     u  e973dd26f0ffa6e46783feaea8f08c83                     Rift coder   \n",
       "26     v  e479e6f38b59afc51a470d1953a6bfc7                          SBert   \n",
       "27     w  dd90e2cb7f8e9316ac3796cece9883b5                          SBert   \n",
       "28     x  919de4dd6f25351bcb0223790db1932d              EM German Mistral   \n",
       "29     y  60ea031126f82db8ddbbfecc668315d2            Nomic Embed Text v1   \n",
       "30     z  a5401e7f7e46ed9fcaed5b60a281d547          Nomic Embed Text v1.5   \n",
       "31   zzz  a8c5a783105f87a481543d4ed7d7586d            Qwen2-1.5B-Instruct   \n",
       "\n",
       "                                     filename    filesize requires  \\\n",
       "0         qwen2.5-coder-7b-instruct-q4_0.gguf  4431390720    3.6.0   \n",
       "1          Meta-Llama-3-8B-Instruct.Q4_0.gguf  4661724384    2.7.1   \n",
       "2       DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf  4444121056    3.8.0   \n",
       "3      DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf  8544267680    3.8.0   \n",
       "4      DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf  4675894112    3.8.0   \n",
       "5     DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf  1068807776    3.8.0   \n",
       "6             Llama-3.2-3B-Instruct-Q4_0.gguf  1921909280    3.4.0   \n",
       "7             Llama-3.2-1B-Instruct-Q4_0.gguf   773025920    3.4.0   \n",
       "8      Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf  4108928000    2.7.1   \n",
       "9          mistral-7b-instruct-v0.1.Q4_0.gguf  4108916384    2.5.0   \n",
       "10  Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf  4661212096    3.1.1   \n",
       "11        mistral-7b-openorca.gguf2.Q4_0.gguf  4108928128    2.7.1   \n",
       "12            gpt4all-falcon-newbpe-q4_0.gguf  4210994112    2.6.0   \n",
       "13                        orca-2-7b.Q4_0.gguf  3825824192    2.5.2   \n",
       "14                       orca-2-13b.Q4_0.gguf  7365856064    2.5.2   \n",
       "15                wizardlm-13b-v1.2.Q4_0.gguf  7365834624    2.5.0   \n",
       "16                  ghost-7b-v0.9.1-Q4_0.gguf  4108916960    2.7.1   \n",
       "17           nous-hermes-llama2-13b.Q4_0.gguf  7366062080    2.5.0   \n",
       "18               gpt4all-13b-snoozy-q4_0.gguf  7365834624    2.5.0   \n",
       "19               mpt-7b-chat-newbpe-q4_0.gguf  3912373472    2.7.1   \n",
       "20                mpt-7b-chat.gguf4.Q4_0.gguf  3796178112    2.7.3   \n",
       "21           Phi-3-mini-4k-instruct.Q4_0.gguf  2176181568    2.7.1   \n",
       "22               orca-mini-3b-gguf2-q4_0.gguf  1979946720    2.5.0   \n",
       "23       replit-code-v1_5-3b-newbpe-q4_0.gguf  1953055104    2.6.0   \n",
       "24                 starcoder-newbpe-q4_0.gguf  8987411904    2.6.0   \n",
       "25                 rift-coder-v0-7b-q4_0.gguf  3825903776    2.5.0   \n",
       "26                  all-MiniLM-L6-v2-f16.gguf    45887744    2.5.0   \n",
       "27            all-MiniLM-L6-v2.gguf2.f16.gguf    45949216    2.7.4   \n",
       "28            em_german_mistral_v01.Q4_0.gguf  4108916352    2.5.0   \n",
       "29               nomic-embed-text-v1.f16.gguf   274290560    2.7.4   \n",
       "30             nomic-embed-text-v1.5.f16.gguf   274290560    2.7.4   \n",
       "31              qwen2-1_5b-instruct-q4_0.gguf   937532800      3.0   \n",
       "\n",
       "   ramrequired   parameters quant       type  \\\n",
       "0            8    8 billion  q4_0      qwen2   \n",
       "1            8    8 billion  q4_0     LLaMA3   \n",
       "2            8    7 billion  q4_0   deepseek   \n",
       "3           16   14 billion  q4_0   deepseek   \n",
       "4            8    8 billion  q4_0   deepseek   \n",
       "5            3  1.5 billion  q4_0   deepseek   \n",
       "6            4    3 billion  q4_0     LLaMA3   \n",
       "7            2    1 billion  q4_0     LLaMA3   \n",
       "8            8    7 billion  q4_0    Mistral   \n",
       "9            8    7 billion  q4_0    Mistral   \n",
       "10           8    8 billion  q4_0     LLaMA3   \n",
       "11           8    7 billion  q4_0    Mistral   \n",
       "12           8    7 billion  q4_0     Falcon   \n",
       "13           8    7 billion  q4_0     LLaMA2   \n",
       "14          16   13 billion  q4_0     LLaMA2   \n",
       "15          16   13 billion  q4_0     LLaMA2   \n",
       "16           8    7 billion  q4_0    Mistral   \n",
       "17          16   13 billion  q4_0     LLaMA2   \n",
       "18          16   13 billion  q4_0      LLaMA   \n",
       "19           8    7 billion  q4_0        MPT   \n",
       "20           8    7 billion  q4_0        MPT   \n",
       "21           4    4 billion  q4_0      Phi-3   \n",
       "22           4    3 billion  q4_0  OpenLLaMa   \n",
       "23           4    3 billion  q4_0     Replit   \n",
       "24           4    7 billion  q4_0  Starcoder   \n",
       "25           8    7 billion  q4_0      LLaMA   \n",
       "26           1   40 million   f16       Bert   \n",
       "27           1   40 million   f16       Bert   \n",
       "28           8    7 billion  q4_0    Mistral   \n",
       "29           1  137 million   f16       Bert   \n",
       "30           1  137 million   f16       Bert   \n",
       "31           3  1.5 billion  q4_0      qwen2   \n",
       "\n",
       "                                          description  \\\n",
       "0   <ul><li>Based on <a href=\"https://huggingface....   \n",
       "1   <ul><li>Fast responses</li><li>Chat based mode...   \n",
       "2   <p>The official Qwen2.5-Math-7B distillation o...   \n",
       "3   <p>The official Qwen2.5-14B distillation of De...   \n",
       "4   <p>The official Llama-3.1-8B distillation of D...   \n",
       "5   <p>The official Qwen2.5-Math-1.5B distillation...   \n",
       "6   <ul><li>Fast responses</li><li>Instruct model<...   \n",
       "7   <ul><li>Fast responses</li><li>Instruct model<...   \n",
       "8   <strong>Good overall fast chat model</strong><...   \n",
       "9   <strong>Strong overall fast instruction follow...   \n",
       "10  <ul><li><strong>For advanced users only. Not r...   \n",
       "11  <strong>Strong overall fast chat model</strong...   \n",
       "12  <strong>Very fast model with good quality</str...   \n",
       "13  <ul><li>Instruction based<li>Trained by Micros...   \n",
       "14  <ul><li>Instruction based<li>Trained by Micros...   \n",
       "15  <strong>Strong overall larger model</strong><b...   \n",
       "16  <strong>Ghost 7B v0.9.1</strong> fast, powerfu...   \n",
       "17  <strong>Extremely good model</strong><br><ul><...   \n",
       "18  <strong>Very good overall model</strong><br><u...   \n",
       "19  <strong>Good model with novel architecture</st...   \n",
       "20  <strong>Good model with novel architecture</st...   \n",
       "21  <ul><li>Very fast responses</li><li>Chat based...   \n",
       "22  <strong>Small version of new model with novel ...   \n",
       "23  <strong>Trained on subset of the Stack</strong...   \n",
       "24  <strong>Trained on subset of the Stack</strong...   \n",
       "25  <strong>Trained on collection of Python and Ty...   \n",
       "26  <strong>LocalDocs text embeddings model</stron...   \n",
       "27  <strong>LocalDocs text embeddings model</stron...   \n",
       "28  <strong>Mistral-based model for German-languag...   \n",
       "29                                nomic-embed-text-v1   \n",
       "30                              nomic-embed-text-v1.5   \n",
       "31  <ul><li>Very fast responses</li><li>Instructio...   \n",
       "\n",
       "                                                  url  \\\n",
       "0   https://huggingface.co/Qwen/Qwen2.5-Coder-7B-I...   \n",
       "1   https://gpt4all.io/models/gguf/Meta-Llama-3-8B...   \n",
       "2   https://huggingface.co/bartowski/DeepSeek-R1-D...   \n",
       "3   https://huggingface.co/bartowski/DeepSeek-R1-D...   \n",
       "4   https://huggingface.co/bartowski/DeepSeek-R1-D...   \n",
       "5   https://huggingface.co/bartowski/DeepSeek-R1-D...   \n",
       "6   https://huggingface.co/bartowski/Llama-3.2-3B-...   \n",
       "7   https://huggingface.co/bartowski/Llama-3.2-1B-...   \n",
       "8   https://huggingface.co/NousResearch/Nous-Herme...   \n",
       "9   https://gpt4all.io/models/gguf/mistral-7b-inst...   \n",
       "10  https://huggingface.co/GPT4All-Community/Meta-...   \n",
       "11  https://gpt4all.io/models/gguf/mistral-7b-open...   \n",
       "12  https://gpt4all.io/models/gguf/gpt4all-falcon-...   \n",
       "13  https://gpt4all.io/models/gguf/orca-2-7b.Q4_0....   \n",
       "14  https://gpt4all.io/models/gguf/orca-2-13b.Q4_0...   \n",
       "15  https://gpt4all.io/models/gguf/wizardlm-13b-v1...   \n",
       "16  https://huggingface.co/lamhieu/ghost-7b-v0.9.1...   \n",
       "17  https://gpt4all.io/models/gguf/nous-hermes-lla...   \n",
       "18  https://gpt4all.io/models/gguf/gpt4all-13b-sno...   \n",
       "19  https://gpt4all.io/models/gguf/mpt-7b-chat-new...   \n",
       "20  https://gpt4all.io/models/gguf/mpt-7b-chat.ggu...   \n",
       "21  https://gpt4all.io/models/gguf/Phi-3-mini-4k-i...   \n",
       "22  https://gpt4all.io/models/gguf/orca-mini-3b-gg...   \n",
       "23  https://gpt4all.io/models/gguf/replit-code-v1_...   \n",
       "24  https://gpt4all.io/models/gguf/starcoder-newbp...   \n",
       "25  https://gpt4all.io/models/gguf/rift-coder-v0-7...   \n",
       "26  https://gpt4all.io/models/gguf/all-MiniLM-L6-v...   \n",
       "27  https://gpt4all.io/models/gguf/all-MiniLM-L6-v...   \n",
       "28  https://huggingface.co/TheBloke/em_german_mist...   \n",
       "29  https://gpt4all.io/models/gguf/nomic-embed-tex...   \n",
       "30  https://gpt4all.io/models/gguf/nomic-embed-tex...   \n",
       "31  https://huggingface.co/Qwen/Qwen2-1.5B-Instruc...   \n",
       "\n",
       "                                         chatTemplate  \\\n",
       "0   {{- '<|im_start|>system\\n' }}\\n{% if toolList|...   \n",
       "1   {%- set loop_messages = messages %}\\n{%- for m...   \n",
       "2   {%- if not add_generation_prompt is defined %}...   \n",
       "3   {%- if not add_generation_prompt is defined %}...   \n",
       "4   {%- if not add_generation_prompt is defined %}...   \n",
       "5   {%- if not add_generation_prompt is defined %}...   \n",
       "6   {{- bos_token }}\\n{%- set date_string = strfti...   \n",
       "7   {{- bos_token }}\\n{%- set date_string = strfti...   \n",
       "8   {%- for message in messages %}\\n    {{- '<|im_...   \n",
       "9   {%- if messages[0]['role'] == 'system' %}\\n   ...   \n",
       "10  {%- set loop_messages = messages %}\\n{%- for m...   \n",
       "11  {%- for message in messages %}\\n    {{- '<|im_...   \n",
       "12  {%- if messages[0]['role'] == 'system' %}\\n   ...   \n",
       "13  {%- for message in messages %}\\n    {{- '<|im_...   \n",
       "14  {%- for message in messages %}\\n    {{- '<|im_...   \n",
       "15  {%- if messages[0]['role'] == 'system' %}\\n   ...   \n",
       "16  {%- for message in messages %}\\n    {%- if mes...   \n",
       "17  {%- if messages[0]['role'] == 'system' %}\\n   ...   \n",
       "18  {%- if messages[0]['role'] == 'system' %}\\n   ...   \n",
       "19  {%- for message in messages %}\\n    {{- '<|im_...   \n",
       "20  {%- for message in messages %}\\n    {{- '<|im_...   \n",
       "21  {{- bos_token }}\\n{%- for message in messages ...   \n",
       "22  {%- if messages[0]['role'] == 'system' %}\\n   ...   \n",
       "23                                               None   \n",
       "24                                               None   \n",
       "25                                               None   \n",
       "26                                               None   \n",
       "27                                               None   \n",
       "28  {%- if messages[0]['role'] == 'system' %}\\n   ...   \n",
       "29                                               None   \n",
       "30                                               None   \n",
       "31  {%- for message in messages %}\\n    {%- if loo...   \n",
       "\n",
       "                                         systemPrompt  \\\n",
       "0                                                       \n",
       "1                                                       \n",
       "2                                                 NaN   \n",
       "3                                                 NaN   \n",
       "4                                                 NaN   \n",
       "5                                                 NaN   \n",
       "6   <|start_header_id|>system<|end_header_id|>\\nCu...   \n",
       "7   <|start_header_id|>system<|end_header_id|>\\nCu...   \n",
       "8                                                       \n",
       "9                                                       \n",
       "10  <|start_header_id|>system<|end_header_id|>\\nCu...   \n",
       "11  <|im_start|>system\\nYou are MistralOrca, a lar...   \n",
       "12                                                      \n",
       "13                                                      \n",
       "14                                                      \n",
       "15                                                      \n",
       "16  <|system|>\\nYou are Ghost created by Lam Hieu....   \n",
       "17                                                      \n",
       "18                                                      \n",
       "19  <|im_start|>system\\n- You are a helpful assist...   \n",
       "20  <|im_start|>system\\n- You are a helpful assist...   \n",
       "21                                                      \n",
       "22  ### System:\\nYou are an AI assistant that foll...   \n",
       "23                                                      \n",
       "24                                                      \n",
       "25                                                      \n",
       "26                                                      \n",
       "27                                                NaN   \n",
       "28                Du bist ein hilfreicher Assistent.    \n",
       "29                                                      \n",
       "30                                                      \n",
       "31  <|im_start|>system\\nBelow is an instruction th...   \n",
       "\n",
       "                                       promptTemplate  \\\n",
       "0                                                 NaN   \n",
       "1   <|start_header_id|>user<|end_header_id|>\\n\\n%1...   \n",
       "2                                                 NaN   \n",
       "3                                                 NaN   \n",
       "4                                                 NaN   \n",
       "5                                                 NaN   \n",
       "6   <|start_header_id|>user<|end_header_id|>\\n\\n%1...   \n",
       "7   <|start_header_id|>user<|end_header_id|>\\n\\n%1...   \n",
       "8   <|im_start|>user\\n%1<|im_end|>\\n<|im_start|>as...   \n",
       "9                                   [INST] %1 [/INST]   \n",
       "10  <|start_header_id|>user<|end_header_id|>\\n\\n%1...   \n",
       "11  <|im_start|>user\\n%1<|im_end|>\\n<|im_start|>as...   \n",
       "12            ### Instruction:\\n%1\\n\\n### Response:\\n   \n",
       "13                                                NaN   \n",
       "14                                                NaN   \n",
       "15                                                NaN   \n",
       "16          <|user|>\\n%1</s>\\n<|assistant|>\\n%2</s>\\n   \n",
       "17            ### Instruction:\\n%1\\n\\n### Response:\\n   \n",
       "18                                                NaN   \n",
       "19  <|im_start|>user\\n%1<|im_end|>\\n<|im_start|>as...   \n",
       "20  <|im_start|>user\\n%1<|im_end|>\\n<|im_start|>as...   \n",
       "21    <|user|>\\n%1<|end|>\\n<|assistant|>\\n%2<|end|>\\n   \n",
       "22                   ### User:\\n%1\\n\\n### Response:\\n   \n",
       "23                                                 %1   \n",
       "24                                                 %1   \n",
       "25                                                 %1   \n",
       "26                                                NaN   \n",
       "27                                                NaN   \n",
       "28                               USER: %1 ASSISTANT:    \n",
       "29                                                NaN   \n",
       "30                                                NaN   \n",
       "31  <|im_start|>user\\n%1<|im_end|>\\n<|im_start|>as...   \n",
       "\n",
       "                                            sha256sum  \\\n",
       "0                                                 NaN   \n",
       "1                                                 NaN   \n",
       "2   5cd4ee65211770f1d99b4f6f4951780b9ef40e29314bd6...   \n",
       "3   906b3382f2680f4ce845459b4a122e904002b075238080...   \n",
       "4   0eb93e436ac8beec18aceb958c120d282cb2cf5451b231...   \n",
       "5   b3af887d0a015b39fab2395e4faf682c1a81a6a3fd09a4...   \n",
       "6                                                 NaN   \n",
       "7                                                 NaN   \n",
       "8                                                 NaN   \n",
       "9                                                 NaN   \n",
       "10                                                NaN   \n",
       "11                                                NaN   \n",
       "12                                                NaN   \n",
       "13                                                NaN   \n",
       "14                                                NaN   \n",
       "15                                                NaN   \n",
       "16                                                NaN   \n",
       "17                                                NaN   \n",
       "18                                                NaN   \n",
       "19                                                NaN   \n",
       "20                                                NaN   \n",
       "21                                                NaN   \n",
       "22                                                NaN   \n",
       "23                                                NaN   \n",
       "24                                                NaN   \n",
       "25                                                NaN   \n",
       "26                                                NaN   \n",
       "27                                                NaN   \n",
       "28                                                NaN   \n",
       "29                                                NaN   \n",
       "30                                                NaN   \n",
       "31                                                NaN   \n",
       "\n",
       "                                        systemMessage removedIn disableGUI  \\\n",
       "0                                                 NaN       NaN        NaN   \n",
       "1                                                 NaN       NaN        NaN   \n",
       "2                                                 NaN       NaN        NaN   \n",
       "3                                                 NaN       NaN        NaN   \n",
       "4                                                 NaN       NaN        NaN   \n",
       "5                                                 NaN       NaN        NaN   \n",
       "6                                                 NaN       NaN        NaN   \n",
       "7                                                 NaN       NaN        NaN   \n",
       "8                                                 NaN       NaN        NaN   \n",
       "9                                                 NaN       NaN        NaN   \n",
       "10                                                NaN       NaN        NaN   \n",
       "11                                                NaN       NaN        NaN   \n",
       "12                                                NaN       NaN        NaN   \n",
       "13                                                NaN       NaN        NaN   \n",
       "14                                                NaN       NaN        NaN   \n",
       "15  A chat between a curious user and an artificia...       NaN        NaN   \n",
       "16  You are Ghost created by Lam Hieu. You are a h...       NaN        NaN   \n",
       "17                                                NaN       NaN        NaN   \n",
       "18  Below is an instruction that describes a task....       NaN        NaN   \n",
       "19                                                NaN     2.7.3        NaN   \n",
       "20                                                NaN       NaN        NaN   \n",
       "21                                                NaN       NaN        NaN   \n",
       "22                                                NaN       NaN        NaN   \n",
       "23                                                NaN       NaN       true   \n",
       "24                                                NaN       NaN       true   \n",
       "25                                                NaN       NaN       true   \n",
       "26                                                NaN     2.7.4       true   \n",
       "27                                                NaN     3.0.0        NaN   \n",
       "28                 Du bist ein hilfreicher Assistent.       NaN        NaN   \n",
       "29                                                NaN       NaN       true   \n",
       "30                                                NaN       NaN       true   \n",
       "31                                                NaN       NaN        NaN   \n",
       "\n",
       "   embeddingModel  \n",
       "0             NaN  \n",
       "1             NaN  \n",
       "2             NaN  \n",
       "3             NaN  \n",
       "4             NaN  \n",
       "5             NaN  \n",
       "6             NaN  \n",
       "7             NaN  \n",
       "8             NaN  \n",
       "9             NaN  \n",
       "10            NaN  \n",
       "11            NaN  \n",
       "12            NaN  \n",
       "13            NaN  \n",
       "14            NaN  \n",
       "15            NaN  \n",
       "16            NaN  \n",
       "17            NaN  \n",
       "18            NaN  \n",
       "19            NaN  \n",
       "20            NaN  \n",
       "21            NaN  \n",
       "22            NaN  \n",
       "23            NaN  \n",
       "24            NaN  \n",
       "25            NaN  \n",
       "26           True  \n",
       "27           True  \n",
       "28            NaN  \n",
       "29           True  \n",
       "30           True  \n",
       "31            NaN  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b8c071ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['order', 'md5sum', 'name', 'filename', 'filesize', 'requires',\n",
       "       'ramrequired', 'parameters', 'quant', 'type', 'description', 'url',\n",
       "       'chatTemplate', 'systemPrompt', 'promptTemplate', 'sha256sum',\n",
       "       'systemMessage', 'removedIn', 'disableGUI', 'embeddingModel'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the columns of the DataFrame\n",
    "Models_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5b702f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 20)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dimensions of the DataFrame\n",
    "Models_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "be26b486-4494-4bf4-84eb-bfbe1989a90b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order</th>\n",
       "      <th>md5sum</th>\n",
       "      <th>name</th>\n",
       "      <th>filename</th>\n",
       "      <th>filesize</th>\n",
       "      <th>requires</th>\n",
       "      <th>ramrequired</th>\n",
       "      <th>parameters</th>\n",
       "      <th>quant</th>\n",
       "      <th>type</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>chatTemplate</th>\n",
       "      <th>systemPrompt</th>\n",
       "      <th>promptTemplate</th>\n",
       "      <th>sha256sum</th>\n",
       "      <th>systemMessage</th>\n",
       "      <th>removedIn</th>\n",
       "      <th>disableGUI</th>\n",
       "      <th>embeddingModel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aa4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DeepSeek-R1-Distill-Qwen-1.5B</td>\n",
       "      <td>DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf</td>\n",
       "      <td>1068807776</td>\n",
       "      <td>3.8.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.5 billion</td>\n",
       "      <td>q4_0</td>\n",
       "      <td>deepseek</td>\n",
       "      <td>&lt;p&gt;The official Qwen2.5-Math-1.5B distillation...</td>\n",
       "      <td>https://huggingface.co/bartowski/DeepSeek-R1-D...</td>\n",
       "      <td>{%- if not add_generation_prompt is defined %}...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b3af887d0a015b39fab2395e4faf682c1a81a6a3fd09a4...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>c</td>\n",
       "      <td>48ff0243978606fdba19d899b77802fc</td>\n",
       "      <td>Llama 3.2 1B Instruct</td>\n",
       "      <td>Llama-3.2-1B-Instruct-Q4_0.gguf</td>\n",
       "      <td>773025920</td>\n",
       "      <td>3.4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1 billion</td>\n",
       "      <td>q4_0</td>\n",
       "      <td>LLaMA3</td>\n",
       "      <td>&lt;ul&gt;&lt;li&gt;Fast responses&lt;/li&gt;&lt;li&gt;Instruct model&lt;...</td>\n",
       "      <td>https://huggingface.co/bartowski/Llama-3.2-1B-...</td>\n",
       "      <td>{{- bos_token }}\\n{%- set date_string = strfti...</td>\n",
       "      <td>&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\nCu...</td>\n",
       "      <td>&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\n%1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>v</td>\n",
       "      <td>e479e6f38b59afc51a470d1953a6bfc7</td>\n",
       "      <td>SBert</td>\n",
       "      <td>all-MiniLM-L6-v2-f16.gguf</td>\n",
       "      <td>45887744</td>\n",
       "      <td>2.5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40 million</td>\n",
       "      <td>f16</td>\n",
       "      <td>Bert</td>\n",
       "      <td>&lt;strong&gt;LocalDocs text embeddings model&lt;/stron...</td>\n",
       "      <td>https://gpt4all.io/models/gguf/all-MiniLM-L6-v...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.7.4</td>\n",
       "      <td>true</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>w</td>\n",
       "      <td>dd90e2cb7f8e9316ac3796cece9883b5</td>\n",
       "      <td>SBert</td>\n",
       "      <td>all-MiniLM-L6-v2.gguf2.f16.gguf</td>\n",
       "      <td>45949216</td>\n",
       "      <td>2.7.4</td>\n",
       "      <td>1</td>\n",
       "      <td>40 million</td>\n",
       "      <td>f16</td>\n",
       "      <td>Bert</td>\n",
       "      <td>&lt;strong&gt;LocalDocs text embeddings model&lt;/stron...</td>\n",
       "      <td>https://gpt4all.io/models/gguf/all-MiniLM-L6-v...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>y</td>\n",
       "      <td>60ea031126f82db8ddbbfecc668315d2</td>\n",
       "      <td>Nomic Embed Text v1</td>\n",
       "      <td>nomic-embed-text-v1.f16.gguf</td>\n",
       "      <td>274290560</td>\n",
       "      <td>2.7.4</td>\n",
       "      <td>1</td>\n",
       "      <td>137 million</td>\n",
       "      <td>f16</td>\n",
       "      <td>Bert</td>\n",
       "      <td>nomic-embed-text-v1</td>\n",
       "      <td>https://gpt4all.io/models/gguf/nomic-embed-tex...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>true</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>z</td>\n",
       "      <td>a5401e7f7e46ed9fcaed5b60a281d547</td>\n",
       "      <td>Nomic Embed Text v1.5</td>\n",
       "      <td>nomic-embed-text-v1.5.f16.gguf</td>\n",
       "      <td>274290560</td>\n",
       "      <td>2.7.4</td>\n",
       "      <td>1</td>\n",
       "      <td>137 million</td>\n",
       "      <td>f16</td>\n",
       "      <td>Bert</td>\n",
       "      <td>nomic-embed-text-v1.5</td>\n",
       "      <td>https://gpt4all.io/models/gguf/nomic-embed-tex...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>true</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>zzz</td>\n",
       "      <td>a8c5a783105f87a481543d4ed7d7586d</td>\n",
       "      <td>Qwen2-1.5B-Instruct</td>\n",
       "      <td>qwen2-1_5b-instruct-q4_0.gguf</td>\n",
       "      <td>937532800</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.5 billion</td>\n",
       "      <td>q4_0</td>\n",
       "      <td>qwen2</td>\n",
       "      <td>&lt;ul&gt;&lt;li&gt;Very fast responses&lt;/li&gt;&lt;li&gt;Instructio...</td>\n",
       "      <td>https://huggingface.co/Qwen/Qwen2-1.5B-Instruc...</td>\n",
       "      <td>{%- for message in messages %}\\n    {%- if loo...</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nBelow is an instruction th...</td>\n",
       "      <td>&lt;|im_start|&gt;user\\n%1&lt;|im_end|&gt;\\n&lt;|im_start|&gt;as...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   order                            md5sum                           name  \\\n",
       "5    aa4                               NaN  DeepSeek-R1-Distill-Qwen-1.5B   \n",
       "7      c  48ff0243978606fdba19d899b77802fc          Llama 3.2 1B Instruct   \n",
       "26     v  e479e6f38b59afc51a470d1953a6bfc7                          SBert   \n",
       "27     w  dd90e2cb7f8e9316ac3796cece9883b5                          SBert   \n",
       "29     y  60ea031126f82db8ddbbfecc668315d2            Nomic Embed Text v1   \n",
       "30     z  a5401e7f7e46ed9fcaed5b60a281d547          Nomic Embed Text v1.5   \n",
       "31   zzz  a8c5a783105f87a481543d4ed7d7586d            Qwen2-1.5B-Instruct   \n",
       "\n",
       "                                   filename    filesize requires ramrequired  \\\n",
       "5   DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf  1068807776    3.8.0           3   \n",
       "7           Llama-3.2-1B-Instruct-Q4_0.gguf   773025920    3.4.0           2   \n",
       "26                all-MiniLM-L6-v2-f16.gguf    45887744    2.5.0           1   \n",
       "27          all-MiniLM-L6-v2.gguf2.f16.gguf    45949216    2.7.4           1   \n",
       "29             nomic-embed-text-v1.f16.gguf   274290560    2.7.4           1   \n",
       "30           nomic-embed-text-v1.5.f16.gguf   274290560    2.7.4           1   \n",
       "31            qwen2-1_5b-instruct-q4_0.gguf   937532800      3.0           3   \n",
       "\n",
       "     parameters quant      type  \\\n",
       "5   1.5 billion  q4_0  deepseek   \n",
       "7     1 billion  q4_0    LLaMA3   \n",
       "26   40 million   f16      Bert   \n",
       "27   40 million   f16      Bert   \n",
       "29  137 million   f16      Bert   \n",
       "30  137 million   f16      Bert   \n",
       "31  1.5 billion  q4_0     qwen2   \n",
       "\n",
       "                                          description  \\\n",
       "5   <p>The official Qwen2.5-Math-1.5B distillation...   \n",
       "7   <ul><li>Fast responses</li><li>Instruct model<...   \n",
       "26  <strong>LocalDocs text embeddings model</stron...   \n",
       "27  <strong>LocalDocs text embeddings model</stron...   \n",
       "29                                nomic-embed-text-v1   \n",
       "30                              nomic-embed-text-v1.5   \n",
       "31  <ul><li>Very fast responses</li><li>Instructio...   \n",
       "\n",
       "                                                  url  \\\n",
       "5   https://huggingface.co/bartowski/DeepSeek-R1-D...   \n",
       "7   https://huggingface.co/bartowski/Llama-3.2-1B-...   \n",
       "26  https://gpt4all.io/models/gguf/all-MiniLM-L6-v...   \n",
       "27  https://gpt4all.io/models/gguf/all-MiniLM-L6-v...   \n",
       "29  https://gpt4all.io/models/gguf/nomic-embed-tex...   \n",
       "30  https://gpt4all.io/models/gguf/nomic-embed-tex...   \n",
       "31  https://huggingface.co/Qwen/Qwen2-1.5B-Instruc...   \n",
       "\n",
       "                                         chatTemplate  \\\n",
       "5   {%- if not add_generation_prompt is defined %}...   \n",
       "7   {{- bos_token }}\\n{%- set date_string = strfti...   \n",
       "26                                               None   \n",
       "27                                               None   \n",
       "29                                               None   \n",
       "30                                               None   \n",
       "31  {%- for message in messages %}\\n    {%- if loo...   \n",
       "\n",
       "                                         systemPrompt  \\\n",
       "5                                                 NaN   \n",
       "7   <|start_header_id|>system<|end_header_id|>\\nCu...   \n",
       "26                                                      \n",
       "27                                                NaN   \n",
       "29                                                      \n",
       "30                                                      \n",
       "31  <|im_start|>system\\nBelow is an instruction th...   \n",
       "\n",
       "                                       promptTemplate  \\\n",
       "5                                                 NaN   \n",
       "7   <|start_header_id|>user<|end_header_id|>\\n\\n%1...   \n",
       "26                                                NaN   \n",
       "27                                                NaN   \n",
       "29                                                NaN   \n",
       "30                                                NaN   \n",
       "31  <|im_start|>user\\n%1<|im_end|>\\n<|im_start|>as...   \n",
       "\n",
       "                                            sha256sum systemMessage removedIn  \\\n",
       "5   b3af887d0a015b39fab2395e4faf682c1a81a6a3fd09a4...           NaN       NaN   \n",
       "7                                                 NaN           NaN       NaN   \n",
       "26                                                NaN           NaN     2.7.4   \n",
       "27                                                NaN           NaN     3.0.0   \n",
       "29                                                NaN           NaN       NaN   \n",
       "30                                                NaN           NaN       NaN   \n",
       "31                                                NaN           NaN       NaN   \n",
       "\n",
       "   disableGUI embeddingModel  \n",
       "5         NaN            NaN  \n",
       "7         NaN            NaN  \n",
       "26       true           True  \n",
       "27        NaN           True  \n",
       "29       true           True  \n",
       "30       true           True  \n",
       "31        NaN            NaN  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter models that require less than 4 GB of RAM\n",
    "# Convert 'ramrequired' to numeric and filter to ramrequired < 4\n",
    "Models_df[Models_df[\"ramrequired\"].astype(float) < 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b05412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b00b03b-a5bd-45a7-a9e1-e806563bd1ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
