{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4f8bed4",
   "metadata": {},
   "source": [
    "# GPT4All - setup by downloading models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d53b1d",
   "metadata": {},
   "source": [
    "##  Teaching LLM workflow by using open source models and GPT4All\n",
    "GPT4all is a framework to source models and handle Jupyter workflow and have them  work within the confines of limited compute eg like a personal computer or a cloud based server like we use for instruction.  \n",
    "\n",
    "### Shared Filesystem \n",
    "\n",
    "In the setup where I was teaching I used this notebook to download models from Huggingface and I put them in a shared-readwrite folder, where the students could access them on Jupyterhub.  I was using a Jupyterhub for teaching that had a shared folder system.  \n",
    "\n",
    "Your use case may vary \n",
    "- shared read write\n",
    "- each student downloads own models\n",
    "- download models to local "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4031df85-3599-4e95-a59d-f476cb36decd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that your python environment has gpt4all package installed.\n",
    "try:\n",
    "    from gpt4all import GPT4All\n",
    "except ImportError:\n",
    "    %pip install gpt4all\n",
    "    from gpt4all import GPT4All\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93d330d",
   "metadata": {},
   "source": [
    "## Which model to download\n",
    "In the use case for teaching on a Juptyerhub with a CPU, I was looking for **small models**, \n",
    " - ~1bn parameters\n",
    " - quantized (Weights have 4 decimal places instead of 10 )\n",
    "\n",
    "\n",
    "(This info is as of the writing of this notebook in May/June 2025 and this info is changing rapidly) \n",
    "\n",
    "\n",
    "You can explore the world of models at :\n",
    "[Hugging Face Model List](https://huggingface.co/models)\n",
    "\n",
    "GPT4All is using a subset of these models - Here is the description from their [documentation](https://docs.gpt4all.io/gpt4all_desktop/models.html#explore-models):\n",
    "\n",
    "- Many LLMs are available at various sizes, quantizations, and licenses.\n",
    "\n",
    "- LLMs with more parameters tend to be better at coherently responding to instructions\n",
    "\n",
    "- LLMs with a smaller quantization (e.g. 4bit instead of 16bit) are much faster and less memory intensive, and tend to have slightly worse performance\n",
    "\n",
    "- Licenses vary in their terms for personal and commercial use\n",
    "\n",
    "\n",
    "\n",
    "Five that I picked to download are:\n",
    "- `DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf`\n",
    "- `Phi-3-mini-4k-instruct.Q4_0.gguf`\n",
    "- `Llama-3.2-1B-Instruct-Q4_0.gguf`\t\t \n",
    "- `qwen2-1_5b-instruct-q4_0.gguf`\n",
    "- `mistral-7b-instruct-v0.1.Q4_0.gguf`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273601bd",
   "metadata": {},
   "source": [
    "The simplest way to download a model is just to call for it in GPT4All and then it downloads it if you dont have it\n",
    "\n",
    "Don't worry if you get `llama_model_load: error loading model: error loading model vocabulary: unknown pre-tokenizer type: 'deepseek-r1-qwen'`  thats about access to GPUs which we don't have in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f784d27-ea55-4a2f-86e7-d08ff0e5f2f1",
   "metadata": {},
   "source": [
    "## Let's check out our local filesystem path and where we will download the files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0005c223-3f01-4478-92d6-3c2de6bb1a9d",
   "metadata": {},
   "source": [
    "### Approach 1 -  if a Shared Hub is being used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1475f454-9d2a-45c2-9b98-11e3ee82304b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This only worked for SP 25 instuction on Berkeley Datahub\n",
    "#!ls /home/jovyan/_shared/econ148-readwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e054e688-fa14-4990-aa56-be9aea92614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cal-ICOR workhop Hub?\n",
    "!ls /home/jovyan/shared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595269ab-c7c0-4da5-8110-2907dfc36d86",
   "metadata": {},
   "source": [
    "### Approach 2 -  if a local machine is being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600fff6e-0a22-4d6f-ae3e-8c4c28d6d3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is my local path to a directory called shared-rw\n",
    "!ls shared-rw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640f0889-5886-40ad-bd60-df4745851d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or the full path ( this is on my laptop) \n",
    "!ls /Users/ericvandusen/Documents/GitHub/SmallLM-SP25/shared-rw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c238eb73-62e8-4cee-aeb3-34a81be11218",
   "metadata": {},
   "source": [
    "### Set the path where the models will download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6ce40c-662d-4815-8339-0da6b45c59ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path for Shared Hub\n",
    "path = \"/home/jovyan/shared\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5d6020-693b-4bef-8810-de0d21675bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for Local \n",
    "#path=\"/Users/ericvandusen/Documents/GitHub/SmallLM-SP25/shared-rw\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87d72df-194d-4209-b728-2ae92f5273f6",
   "metadata": {},
   "source": [
    "## Downloading the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f8a73d-e34f-4c52-bd7f-5e70f0b869d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the \"model\" object to which this notebook's code will send conversations & prompts\n",
    "model = GPT4All(\n",
    "    model_name=\"DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf\",\n",
    "    allow_download=True,\n",
    "    model_path=path,\n",
    "    verbose=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e739378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the \"model\" object to which this notebook's code will send conversations & prompts\n",
    "model = GPT4All(\n",
    "    model_name=\"orca-mini-3b-gguf2-q4_0.gguf\",\n",
    "    allow_download=True,\n",
    "    model_path=path,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef346506-e0ee-4f13-be1f-a2090850f57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the \"model\" object to which this notebook's code will send conversations & prompts\n",
    "model = GPT4All(\n",
    "    model_name=\"qwen2-1_5b-instruct-q4_0.gguf\",\n",
    "    allow_download=True,\n",
    "    model_path=path,\n",
    "    verbose=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c741305-e39c-4251-9347-c30f18e80f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show models available in the Hub shared directory. Larger models may run slowly, or not at all.\n",
    "import os\n",
    "print(\"\\n\".join(os.listdir(path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d37c9e6-936a-40be-b9a5-a213552b1b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls \"{path}\" -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f856c4d5",
   "metadata": {},
   "source": [
    "## Bonus Searching for models from a database \n",
    "\n",
    " - We can go to GPT4All database\n",
    " - Make that database into a pandas dataframe\n",
    " - Filter to pick nodels we want\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06680376-15c4-4cf3-aecb-c802e474352a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9a86bc-c0a5-461b-bb3a-5f1a33d89398",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load JSON from the GPT4All models repository\n",
    "#Small curated list\n",
    "url = \"https://gpt4all.io/models/models3.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ce2234",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = requests.get(url).json()\n",
    "# Convert to DataFrame\n",
    "Models_df = pd.DataFrame(models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614239e5-eb14-4594-9d24-c7c3c0fda472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceec60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c071ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the columns of the DataFrame\n",
    "Models_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b702f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dimensions of the DataFrame\n",
    "Models_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be26b486-4494-4bf4-84eb-bfbe1989a90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter models that require less than 4 GB of RAM\n",
    "# Convert 'ramrequired' to numeric and filter to ramrequired < 4\n",
    "Models_df[Models_df[\"ramrequired\"].astype(float) < 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b05412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b00b03b-a5bd-45a7-a9e1-e806563bd1ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12692ccb-630f-4f32-b540-6ea324e81974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6234c4d-eded-4111-8552-20a358c7cc6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a27cc56-f406-423f-adf2-c96e7a2f1461",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
